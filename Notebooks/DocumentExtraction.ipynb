{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document extraction\n",
    "\n",
    "This notebook's purpose is to make an in-depth exploration on the best ways to extract information regarding OpenCV. During the first exploratory notebook, documents were extracted from OpenCV's documentation using a recursive URL loader. One of the main challenges of this RAG is to make information Python-specific, which might be difficult given that OpenCV is written in C++, with `opencv-python` being the library available as a wrapper with bindings for Python. Most of the solutions to overcome this challenge may come in the preprocessing and prompt engineering sections of this RAG, but an exploratory task is done during this extraction phase in order to include relevant metadata that may ignore C++-centric code and documentation. Taking this issue into account, in addition to the initial exploration made in the [ExploringOpenCV](/../Notebooks/ExploringOpenCV.ipynb) notebook, this notebook has the following goals:\n",
    "\n",
    "1. Further explore the best way to extract information from OpenCV's documentation.\n",
    "    1. Explore different values for the most relevant `RecursiveUrlLoader`'s parameters. Analyze document number vs extraction velocity.\n",
    "    2. Explore the effectiveness of using multiple loaders instead of a single one, starting from more specific root URLs.\n",
    "    3. For each new extraction idea, compare URL title with [OpenCV Documentation's index](https://docs.opencv.org/4.x/index.html), in order to identify relevance.\n",
    "    4. Record and analyze failed or slow requests, to better refine parameters in `RecursiveUrlLoader`. \n",
    "2. Find new sources for information regarding OpenCV's features.\n",
    "    1. Explore the best way to extract information from [OpenCV tutorials](https://docs.opencv.org/4.x/d9/df8/tutorial_root.html) (probably `RecursiveUrlLoader` is again the best approach). Explore different approaches.\n",
    "    2. Extract information from [LearnOpenCV](https://learnopencv.com). Specially, from its three main guides: \n",
    "        - [Getting Started with OpenCV](https://learnopencv.com/getting-started-with-opencv/)\n",
    "        - [Getting Started with PyTorch](https://learnopencv.com/getting-started-with-pytorch/)\n",
    "        - [Getting Started with Keras & Tensorflow](https://learnopencv.com/getting-started-with-tensorflow-keras/)\n",
    "    In this case, `RecursiveUrlLoader` might not be the best idea. Instead, an initial approach will be to have a tutorial list for each guide, which is easily accessible.\n",
    "    3. Explore the usefulness of including [FreeCodeCamp's OpenCV Course](https://www.youtube.com/watch?v=oXlwWbU8l2o) as part of the knowledge base. This would require a transcript of the video, which might be computationally expensive and there's risk of inaccuracies.\n",
    "3. Create a relevance measurement for each created loader. This tool would compare each document's title and summary to a base document (to be defined) and flag possible irrelevant documents. This step is important for two main concerns regarding data extraction:\n",
    "    - `RecursiveUrlLoader` may extract documents that are not relevant to a person looking for documentation, such as a contact page.\n",
    "    - Some documents might be too C++-oriented, such as the use of CMake to install OpenCV. These should be discarded.\n",
    "4. For each document in each loader, extract relevant metadata that might indicate that the content is not relevant for Python, so that it can be pre-processed in a later stage and converted to the Python equivalent if needed.\n",
    "5. Measure performance indicators for each loader, such as extraction speed, completeness and failures, which will be the final judge regarding the loaders to include in the final version. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by setting up relevant environment variables. In this particular case, it is important to set `USER_AGENT` while web scrapping in order to avoid being blocked during extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:132.0) Gecko/20100101 Firefox/132.0\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "user_agent = os.getenv(\"USER_AGENT\", \"DefaultUserAgent\")\n",
    "print(user_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will focus on exploring the most relevant parameters of `RecursiveUrlLoader`. Besides the `url`, `extractor` and `prevent_outside`, which will not be changed for the purpose of this exploration, we will focus on two parameters that are going to be changed:\n",
    "- `max_depth`\n",
    "- `timeout`\n",
    "\n",
    "We will also register the logs of the extraction process to identify extraction time, document count and possible errors. First, we set up needed imports, logging and parameters to be adjusted, in addition to the `BeautifulSoup` extractor used, which makes the extracted documents human- and LLM-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=\"loader_experiment.log\", level=logging.INFO, \n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Directory to save extracted documents\n",
    "os.makedirs(\"../data/ExtractedDocuments\", exist_ok=True)\n",
    "\n",
    "# Set up parameter combinations to test\n",
    "depth_values = [1, 2, 3, 5, 7]\n",
    "timeout_values = [2, 5, 10, 15]\n",
    "\n",
    "\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then extract all the documents using `RecursiveUrlLoader`. The extracted documents will be saved in the `/data/ExtractedDocuments` path of the project as JSON files. This is a lengthy process, given the recursive nature of the algorithm, so run it at your own risk. It took ~3 hours to run for me. Finally, the extraction results will be saved as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'max_depth': 1, 'timeout': 2, 'doc_count': 1, 'duration': 0.33954763412475586, 'file': '../data/ExtractedDocuments/docs_depth1_timeout2_20241112_130610.json'}\n",
      "1\n",
      "{'max_depth': 1, 'timeout': 5, 'doc_count': 1, 'duration': 0.29752612113952637, 'file': '../data/ExtractedDocuments/docs_depth1_timeout5_20241112_130610.json'}\n",
      "1\n",
      "{'max_depth': 1, 'timeout': 10, 'doc_count': 1, 'duration': 0.4655568599700928, 'file': '../data/ExtractedDocuments/docs_depth1_timeout10_20241112_130610.json'}\n",
      "1\n",
      "{'max_depth': 1, 'timeout': 15, 'doc_count': 1, 'duration': 0.45436692237854004, 'file': '../data/ExtractedDocuments/docs_depth1_timeout15_20241112_130611.json'}\n",
      "81\n",
      "{'max_depth': 2, 'timeout': 2, 'doc_count': 81, 'duration': 44.66263556480408, 'file': '../data/ExtractedDocuments/docs_depth2_timeout2_20241112_130655.json'}\n",
      "81\n",
      "{'max_depth': 2, 'timeout': 5, 'doc_count': 81, 'duration': 41.8696403503418, 'file': '../data/ExtractedDocuments/docs_depth2_timeout5_20241112_130737.json'}\n",
      "81\n",
      "{'max_depth': 2, 'timeout': 10, 'doc_count': 81, 'duration': 41.798401832580566, 'file': '../data/ExtractedDocuments/docs_depth2_timeout10_20241112_130819.json'}\n",
      "81\n",
      "{'max_depth': 2, 'timeout': 15, 'doc_count': 81, 'duration': 42.5337278842926, 'file': '../data/ExtractedDocuments/docs_depth2_timeout15_20241112_130902.json'}\n",
      "1537\n",
      "{'max_depth': 3, 'timeout': 2, 'doc_count': 1537, 'duration': 568.871146440506, 'file': '../data/ExtractedDocuments/docs_depth3_timeout2_20241112_131831.json'}\n",
      "1536\n",
      "{'max_depth': 3, 'timeout': 5, 'doc_count': 1536, 'duration': 582.5020959377289, 'file': '../data/ExtractedDocuments/docs_depth3_timeout5_20241112_132813.json'}\n",
      "1537\n",
      "{'max_depth': 3, 'timeout': 10, 'doc_count': 1537, 'duration': 578.751663684845, 'file': '../data/ExtractedDocuments/docs_depth3_timeout10_20241112_133752.json'}\n",
      "1534\n",
      "{'max_depth': 3, 'timeout': 15, 'doc_count': 1534, 'duration': 652.4447038173676, 'file': '../data/ExtractedDocuments/docs_depth3_timeout15_20241112_134844.json'}\n",
      "2026\n",
      "{'max_depth': 5, 'timeout': 2, 'doc_count': 2026, 'duration': 778.3701250553131, 'file': '../data/ExtractedDocuments/docs_depth5_timeout2_20241112_140143.json'}\n",
      "2026\n",
      "{'max_depth': 5, 'timeout': 5, 'doc_count': 2026, 'duration': 800.413782119751, 'file': '../data/ExtractedDocuments/docs_depth5_timeout5_20241112_141503.json'}\n",
      "2026\n",
      "{'max_depth': 5, 'timeout': 10, 'doc_count': 2026, 'duration': 786.0767018795013, 'file': '../data/ExtractedDocuments/docs_depth5_timeout10_20241112_142809.json'}\n",
      "2026\n",
      "{'max_depth': 5, 'timeout': 15, 'doc_count': 2026, 'duration': 790.3761448860168, 'file': '../data/ExtractedDocuments/docs_depth5_timeout15_20241112_144120.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34612/904634524.py:19: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(html, \"lxml\")\n",
      "/usr/lib/python3.12/html/parser.py:171: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3067\n",
      "{'max_depth': 7, 'timeout': 2, 'doc_count': 3067, 'duration': 1174.5577120780945, 'file': '../data/ExtractedDocuments/docs_depth7_timeout2_20241112_150054.json'}\n",
      "3067\n",
      "{'max_depth': 7, 'timeout': 5, 'doc_count': 3067, 'duration': 1167.2603085041046, 'file': '../data/ExtractedDocuments/docs_depth7_timeout5_20241112_152022.json'}\n",
      "3065\n",
      "{'max_depth': 7, 'timeout': 10, 'doc_count': 3065, 'duration': 1203.1527228355408, 'file': '../data/ExtractedDocuments/docs_depth7_timeout10_20241112_154025.json'}\n",
      "3067\n",
      "{'max_depth': 7, 'timeout': 15, 'doc_count': 3067, 'duration': 1164.0093593597412, 'file': '../data/ExtractedDocuments/docs_depth7_timeout15_20241112_155949.json'}\n",
      "Experiment Results:\n",
      "Depth=1, Timeout=2-> 1 documents in 0.34s, saved to ../data/ExtractedDocuments/docs_depth1_timeout2_20241112_130610.json\n",
      "Depth=1, Timeout=5-> 1 documents in 0.30s, saved to ../data/ExtractedDocuments/docs_depth1_timeout5_20241112_130610.json\n",
      "Depth=1, Timeout=10-> 1 documents in 0.47s, saved to ../data/ExtractedDocuments/docs_depth1_timeout10_20241112_130610.json\n",
      "Depth=1, Timeout=15-> 1 documents in 0.45s, saved to ../data/ExtractedDocuments/docs_depth1_timeout15_20241112_130611.json\n",
      "Depth=2, Timeout=2-> 81 documents in 44.66s, saved to ../data/ExtractedDocuments/docs_depth2_timeout2_20241112_130655.json\n",
      "Depth=2, Timeout=5-> 81 documents in 41.87s, saved to ../data/ExtractedDocuments/docs_depth2_timeout5_20241112_130737.json\n",
      "Depth=2, Timeout=10-> 81 documents in 41.80s, saved to ../data/ExtractedDocuments/docs_depth2_timeout10_20241112_130819.json\n",
      "Depth=2, Timeout=15-> 81 documents in 42.53s, saved to ../data/ExtractedDocuments/docs_depth2_timeout15_20241112_130902.json\n",
      "Depth=3, Timeout=2-> 1537 documents in 568.87s, saved to ../data/ExtractedDocuments/docs_depth3_timeout2_20241112_131831.json\n",
      "Depth=3, Timeout=5-> 1536 documents in 582.50s, saved to ../data/ExtractedDocuments/docs_depth3_timeout5_20241112_132813.json\n",
      "Depth=3, Timeout=10-> 1537 documents in 578.75s, saved to ../data/ExtractedDocuments/docs_depth3_timeout10_20241112_133752.json\n",
      "Depth=3, Timeout=15-> 1534 documents in 652.44s, saved to ../data/ExtractedDocuments/docs_depth3_timeout15_20241112_134844.json\n",
      "Depth=5, Timeout=2-> 2026 documents in 778.37s, saved to ../data/ExtractedDocuments/docs_depth5_timeout2_20241112_140143.json\n",
      "Depth=5, Timeout=5-> 2026 documents in 800.41s, saved to ../data/ExtractedDocuments/docs_depth5_timeout5_20241112_141503.json\n",
      "Depth=5, Timeout=10-> 2026 documents in 786.08s, saved to ../data/ExtractedDocuments/docs_depth5_timeout10_20241112_142809.json\n",
      "Depth=5, Timeout=15-> 2026 documents in 790.38s, saved to ../data/ExtractedDocuments/docs_depth5_timeout15_20241112_144120.json\n",
      "Depth=7, Timeout=2-> 3067 documents in 1174.56s, saved to ../data/ExtractedDocuments/docs_depth7_timeout2_20241112_150054.json\n",
      "Depth=7, Timeout=5-> 3067 documents in 1167.26s, saved to ../data/ExtractedDocuments/docs_depth7_timeout5_20241112_152022.json\n",
      "Depth=7, Timeout=10-> 3065 documents in 1203.15s, saved to ../data/ExtractedDocuments/docs_depth7_timeout10_20241112_154025.json\n",
      "Depth=7, Timeout=15-> 3067 documents in 1164.01s, saved to ../data/ExtractedDocuments/docs_depth7_timeout15_20241112_155949.json\n",
      "\n",
      "Summary saved to extraction_results_summary.json\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from langchain.document_loaders import RecursiveUrlLoader\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# Store results for analysis\n",
    "results = []\n",
    "\n",
    "# Run the parameter tests and save documents\n",
    "for depth in depth_values:\n",
    "    for timeout in timeout_values:\n",
    "        loader = RecursiveUrlLoader(\n",
    "            \"https://docs.opencv.org/4.x/\",\n",
    "            headers={\"User-Agent\": user_agent},\n",
    "            extractor=bs4_extractor,\n",
    "            max_depth=depth,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # Extract documents\n",
    "            documents = loader.load()\n",
    "            duration = time.time() - start_time\n",
    "            doc_count = len(documents)\n",
    "            # Save documents to a JSON file\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"../data/ExtractedDocuments/docs_depth{depth}_timeout{timeout}_{timestamp}.json\"\n",
    "            with open(filename, \"w\") as file:\n",
    "                for doc in documents:\n",
    "                    file.write(doc.json()+'\\n')\n",
    "            logging.info(\n",
    "                f\"Depth={depth}, Timeout={timeout}: \"\n",
    "                f\"Extracted {doc_count} documents in {duration:.2f} seconds.\"\n",
    "            )\n",
    "            result = {\n",
    "                \"max_depth\": depth,\n",
    "                \"timeout\": timeout,\n",
    "                \"doc_count\": doc_count,\n",
    "                \"duration\": duration,\n",
    "                \"file\": filename\n",
    "            }\n",
    "            results.append(result)\n",
    "            print(result)\n",
    "            \n",
    "        except RequestException as e:\n",
    "            logging.error(\n",
    "                f\"Depth={depth}, Timeout={timeout}: \"\n",
    "                f\"Failed with error {str(e)}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"Depth={depth}, Timeout={timeout}: \"\n",
    "                f\"Encountered unexpected error {str(e)}\"\n",
    "            )\n",
    "\n",
    "# Print and save summary of results\n",
    "print(\"Experiment Results:\")\n",
    "for result in results:\n",
    "    print(f\"Depth={result['max_depth']}, Timeout={result['timeout']}\"\n",
    "          f\"-> {result['doc_count']} documents in {result['duration']:.2f}s, saved to {result['file']}\")\n",
    "\n",
    "# Optionally save the results summary to a JSON file\n",
    "summary_filename = \"extraction_results_summary.json\"\n",
    "with open(summary_filename, \"w\") as summary_file:\n",
    "    json.dump(results, summary_file)\n",
    "print(f\"\\nSummary saved to {summary_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
